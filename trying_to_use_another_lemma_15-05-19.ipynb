{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:\\\\Users\\\\vinic\\\\test_directory\\\\tcc\\\\training.1600000.processed.noemoticon.csv'\n",
    "COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Sentiment140 dataset 1.6kk tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH, encoding=\"ISO-8859-1\", names=COLUMNS)\n",
    "df_copy = df\n",
    "df_copy = df_copy.drop(['ids', 'date', 'flag', 'user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all....\n",
       "5       0                      @Kwesidei not the whole crew \n",
       "6       0                                        Need a hug \n",
       "7       0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8       0               @Tatiana_K nope they didn't have it \n",
       "9       0                          @twittera que me muera ? "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting number of tweets and number of targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_counts = df_copy.target.value_counts()\n",
    "#number_of_tweets = df_copy.ids.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Stopwords and Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = punct + stopwords.words('english') + ['rt', 'via', '...', 'http', 'twitpic', 'tinyurl' ,'www']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\\VERSION 1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "# Lemmatize\n",
    "#sentence = \"The striped bats are hanging on their feet for best\"\n",
    "#> 'The striped bat be hang on their foot for best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 1.0\n",
    "def normalizer(tweet):\n",
    "    punct = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    additional_stop_words = ['rt', 'via', '...', 'http', 'twitpic', 'tinyurl' ,'www']\n",
    "    stopword_list = punct + stop_words + additional_stop_words\n",
    "    \n",
    "    tweet = re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet) #removing hashtags and mentions\n",
    "    tweet_ = re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet)\n",
    "    tweet__ = re.sub(\"[^a-zA-Z]\", \" \", tweet_)\n",
    "    lemmatized = lemmatize_with_postag(tweet__)\n",
    "    tokens = nltk.word_tokenize(lemmatized)[2:]\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    filtered_result = list(filter(lambda l: l not in stopword_list, lower_case))\n",
    "    #lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return filtered_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOT DEPRESSIVE Tweets\n",
    "* ##### This tweets are from sentiment140 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_depressive_tweets_df = df_copy[df_copy.target == 4]\n",
    "not_depressive_tweets_df = not_depressive_tweets_df.sample(n=3574, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3574\n"
     ]
    }
   ],
   "source": [
    "print(len(not_depressive_tweets_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEPRESSIVE Tweets\n",
    "* ##### Converting .txt to JSON and importing DEPRESSIVE TWEETS in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_open = Path(\"C:/Users/vinic/test_directory/tcc/Depression-Sentiment-Analysis-Twitter-Data-master\", \"tweetdata.txt\")\n",
    "with open(file_to_open) as datafile:\n",
    "    content = datafile.read()\n",
    "depressive_tweets_df = pd.read_json('[' + content.replace('}\\n', '},') + ']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Creating column with target 0, which the label for DEPRESSIVE tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>0</td>\n",
       "      <td>I never get this much anxiety until I'm home :/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3570</th>\n",
       "      <td>0</td>\n",
       "      <td>Hospital staff warn of more 'adverse outcomes'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3571</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @Jniewalker: มินย้อกถามซอฮยอนว่า depression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @philuhmena: 7 months ago today I was raped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>0</td>\n",
       "      <td>my anxiety is thru the fucking roof.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "3569       0    I never get this much anxiety until I'm home :/\n",
       "3570       0  Hospital staff warn of more 'adverse outcomes'...\n",
       "3571       0  RT @Jniewalker: มินย้อกถามซอฮยอนว่า depression...\n",
       "3572       0  RT @philuhmena: 7 months ago today I was raped...\n",
       "3573       0               my anxiety is thru the fucking roof."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.DataFrame(np.zeros(depressive_tweets_df.shape[0], dtype=int))\n",
    "depressive_tweets_df['target'] = target\n",
    "depressive_tweets_df[['target', 'text']].tail() #showing final DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Checking if the number of samples from each classes are equal on the dataset. They need ot be balanced in order for the algorithm learns how to identify both classes without bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Nice! The number of classes are equal on dataset.\n"
     ]
    }
   ],
   "source": [
    "len_not_depress = len(not_depressive_tweets_df)\n",
    "len_depress = len(depressive_tweets_df)\n",
    "def equal(a,b):\n",
    "    if (int(a)==int(b)):\n",
    "        return print('> Nice! The number of classes are equal on dataset.')\n",
    "    else:\n",
    "        return print('> Number of classes aren\\'t equal on dataset. :(')\n",
    "\n",
    "equal(len_depress, len_not_depress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Concatenating DEPRESSIVE and NOT DEPRESSIVE tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.concat([not_depressive_tweets_df, depressive_tweets_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3574\n",
       "0    3574\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = all_tweets.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying the number of labels by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3574\n",
       "0    3574\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Bond fireeeee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@Hann_Oxx yep ty  omg thank god schools over :L x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Found it! Thanks everyone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@MariaBurnham4 wow! Well done! Having a little...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-02 00:20:43</td>\n",
       "      <td>[15, 126]</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>2017-06-02 00:20:43.320</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 405869972, 'id_str': '405869972', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lunch with the &amp;quot;fam&amp;quot;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  contributors  \\\n",
       "0       4                                     Bond fireeeee            NaN   \n",
       "1       4  @Hann_Oxx yep ty  omg thank god schools over :L x           NaN   \n",
       "2       4                         Found it! Thanks everyone            NaN   \n",
       "3       0  @MariaBurnham4 wow! Well done! Having a little...           NaN   \n",
       "4       4                   Lunch with the &quot;fam&quot;             NaN   \n",
       "\n",
       "  coordinates          created_at display_text_range  \\\n",
       "0         NaN                 NaT                NaN   \n",
       "1         NaN                 NaT                NaN   \n",
       "2         NaN                 NaT                NaN   \n",
       "3        None 2017-06-02 00:20:43          [15, 126]   \n",
       "4         NaN                 NaT                NaN   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0                                                NaN               NaN   \n",
       "1                                                NaN               NaN   \n",
       "2                                                NaN               NaN   \n",
       "3  {'hashtags': [], 'urls': [], 'user_mentions': ...               NaN   \n",
       "4                                                NaN               NaN   \n",
       "\n",
       "  extended_tweet  favorite_count  \\\n",
       "0            NaN             NaN   \n",
       "1            NaN             NaN   \n",
       "2            NaN             NaN   \n",
       "3            NaN             0.0   \n",
       "4            NaN             NaN   \n",
       "\n",
       "                         ...                         quoted_status  \\\n",
       "0                        ...                                   NaN   \n",
       "1                        ...                                   NaN   \n",
       "2                        ...                                   NaN   \n",
       "3                        ...                                   NaN   \n",
       "4                        ...                                   NaN   \n",
       "\n",
       "  quoted_status_id quoted_status_id_str  retweet_count  retweeted  \\\n",
       "0              NaN                  NaN            NaN        NaN   \n",
       "1              NaN                  NaN            NaN        NaN   \n",
       "2              NaN                  NaN            NaN        NaN   \n",
       "3              NaN                  NaN            0.0      False   \n",
       "4              NaN                  NaN            NaN        NaN   \n",
       "\n",
       "  retweeted_status                                             source  \\\n",
       "0              NaN                                                NaN   \n",
       "1              NaN                                                NaN   \n",
       "2              NaN                                                NaN   \n",
       "3              NaN  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4              NaN                                                NaN   \n",
       "\n",
       "             timestamp_ms  truncated  \\\n",
       "0                     NaT        NaN   \n",
       "1                     NaT        NaN   \n",
       "2                     NaT        NaN   \n",
       "3 2017-06-02 00:20:43.320      False   \n",
       "4                     NaT        NaN   \n",
       "\n",
       "                                                user  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  {'id': 405869972, 'id_str': '405869972', 'name...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets['normalized_tweet'] = all_tweets.text.apply(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Bond fireeeee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>@Hann_Oxx yep ty  omg thank god schools over :L x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ty, omg, thank, god, school, l, x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Found it! Thanks everyone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[thanks, everyone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@MariaBurnham4 wow! Well done! Having a little...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-02 00:20:43</td>\n",
       "      <td>[15, 126]</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>2017-06-02 00:20:43.320</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 405869972, 'id_str': '405869972', 'name...</td>\n",
       "      <td>[little, anxiety, prepare, deployment, grade, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Lunch with the &amp;quot;fam&amp;quot;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[quot, fam, quot]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  contributors  \\\n",
       "0       4                                     Bond fireeeee            NaN   \n",
       "1       4  @Hann_Oxx yep ty  omg thank god schools over :L x           NaN   \n",
       "2       4                         Found it! Thanks everyone            NaN   \n",
       "3       0  @MariaBurnham4 wow! Well done! Having a little...           NaN   \n",
       "4       4                   Lunch with the &quot;fam&quot;             NaN   \n",
       "\n",
       "  coordinates          created_at display_text_range  \\\n",
       "0         NaN                 NaT                NaN   \n",
       "1         NaN                 NaT                NaN   \n",
       "2         NaN                 NaT                NaN   \n",
       "3        None 2017-06-02 00:20:43          [15, 126]   \n",
       "4         NaN                 NaT                NaN   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0                                                NaN               NaN   \n",
       "1                                                NaN               NaN   \n",
       "2                                                NaN               NaN   \n",
       "3  {'hashtags': [], 'urls': [], 'user_mentions': ...               NaN   \n",
       "4                                                NaN               NaN   \n",
       "\n",
       "  extended_tweet  favorite_count  \\\n",
       "0            NaN             NaN   \n",
       "1            NaN             NaN   \n",
       "2            NaN             NaN   \n",
       "3            NaN             0.0   \n",
       "4            NaN             NaN   \n",
       "\n",
       "                         ...                         quoted_status_id  \\\n",
       "0                        ...                                      NaN   \n",
       "1                        ...                                      NaN   \n",
       "2                        ...                                      NaN   \n",
       "3                        ...                                      NaN   \n",
       "4                        ...                                      NaN   \n",
       "\n",
       "  quoted_status_id_str retweet_count  retweeted  retweeted_status  \\\n",
       "0                  NaN           NaN        NaN               NaN   \n",
       "1                  NaN           NaN        NaN               NaN   \n",
       "2                  NaN           NaN        NaN               NaN   \n",
       "3                  NaN           0.0      False               NaN   \n",
       "4                  NaN           NaN        NaN               NaN   \n",
       "\n",
       "                                              source            timestamp_ms  \\\n",
       "0                                                NaN                     NaT   \n",
       "1                                                NaN                     NaT   \n",
       "2                                                NaN                     NaT   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r... 2017-06-02 00:20:43.320   \n",
       "4                                                NaN                     NaT   \n",
       "\n",
       "   truncated                                               user  \\\n",
       "0        NaN                                                NaN   \n",
       "1        NaN                                                NaN   \n",
       "2        NaN                                                NaN   \n",
       "3      False  {'id': 405869972, 'id_str': '405869972', 'name...   \n",
       "4        NaN                                                NaN   \n",
       "\n",
       "                                    normalized_tweet  \n",
       "0                                                 []  \n",
       "1                [ty, omg, thank, god, school, l, x]  \n",
       "2                                 [thanks, everyone]  \n",
       "3  [little, anxiety, prepare, deployment, grade, ...  \n",
       "4                                  [quot, fam, quot]  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating vector of bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'text', 'contributors', 'coordinates', 'created_at',\n",
       "       'display_text_range', 'entities', 'extended_entities', 'extended_tweet',\n",
       "       'favorite_count', 'favorited', 'filter_level', 'geo', 'id', 'id_str',\n",
       "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
       "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
       "       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place',\n",
       "       'possibly_sensitive', 'quoted_status', 'quoted_status_id',\n",
       "       'quoted_status_id_str', 'retweet_count', 'retweeted',\n",
       "       'retweeted_status', 'source', 'timestamp_ms', 'truncated', 'user',\n",
       "       'normalized_tweet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   []\n",
       "1    [ty omg, omg thank, thank god, god school, sch...\n",
       "2                                    [thanks everyone]\n",
       "3    [little anxiety, anxiety prepare, prepare depl...\n",
       "4                                 [quot fam, fam quot]\n",
       "Name: grams, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "def ngrams(input_list):\n",
    "    #onegrams = input_list\n",
    "    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n",
    "    #trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n",
    "    return bigrams\n",
    "\n",
    "all_tweets['grams'] = all_tweets.normalized_tweet.apply(ngrams)\n",
    "all_tweets['grams'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now some counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def count_words(input):\n",
    "    cnt = collections.Counter()\n",
    "    for row in input:\n",
    "        for word in row:\n",
    "            cnt[word] += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the 20 most common n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEPRESSIVE Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mental health', 558),\n",
       " ('resting bitch', 208),\n",
       " ('bitch face', 208),\n",
       " ('want friend', 207),\n",
       " ('friend w', 207),\n",
       " ('w ppl', 207),\n",
       " ('ppl ppl', 207),\n",
       " ('ppl think', 207),\n",
       " ('think ur', 207),\n",
       " ('ur mean', 207),\n",
       " ('mean unapproachable', 207),\n",
       " ('unapproachable cu', 207),\n",
       " ('cu u', 207),\n",
       " ('u resting', 207),\n",
       " ('face soci', 207),\n",
       " ('go miss', 186),\n",
       " ('think go', 185),\n",
       " ('miss eighth', 185),\n",
       " ('eighth grade', 185),\n",
       " ('grade tough', 185)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[(all_tweets.target == 0)][['grams']].apply(count_words)['grams'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOT DEPRESSIVE Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gon na', 36),\n",
       " ('last night', 19),\n",
       " ('wan na', 14),\n",
       " ('look like', 14),\n",
       " ('let know', 13),\n",
       " ('good luck', 12),\n",
       " ('wait see', 12),\n",
       " ('good night', 11),\n",
       " ('day use', 10),\n",
       " ('much good', 10),\n",
       " ('com add', 9),\n",
       " ('add everyone', 9),\n",
       " ('everyone train', 9),\n",
       " ('train pay', 9),\n",
       " ('pay vip', 9),\n",
       " ('cant wait', 9),\n",
       " ('good day', 9),\n",
       " ('first time', 8),\n",
       " ('feel well', 8),\n",
       " ('new song', 8)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[(all_tweets.target == 4)][['grams']].apply(count_words)['grams'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = count_vectorizer.fit_transform(all_tweets.text)\n",
    "indexed_data = hstack((np.array(range(0,vectorized_data.shape[0]))[:,None], vectorized_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bond': 7842,\n",
       " 'fireeeee': 18221,\n",
       " 'bond fireeeee': 7843,\n",
       " 'hann_oxx': 22175,\n",
       " 'yep': 59926,\n",
       " 'ty': 55071,\n",
       " 'omg': 37847,\n",
       " 'thank': 50127,\n",
       " 'god': 20679,\n",
       " 'schools': 44994,\n",
       " 'over': 38960,\n",
       " 'hann_oxx yep': 22176,\n",
       " 'yep ty': 59937,\n",
       " 'ty omg': 55074,\n",
       " 'omg thank': 37863,\n",
       " 'thank god': 50132,\n",
       " 'god schools': 20697,\n",
       " 'schools over': 45002,\n",
       " 'found': 19108,\n",
       " 'it': 26785,\n",
       " 'thanks': 50152,\n",
       " 'everyone': 16995,\n",
       " 'found it': 19114,\n",
       " 'it thanks': 27075,\n",
       " 'thanks everyone': 50170,\n",
       " 'mariaburnham4': 32129,\n",
       " 'wow': 59263,\n",
       " 'well': 57414,\n",
       " 'done': 15315,\n",
       " 'having': 22745,\n",
       " 'little': 30527,\n",
       " 'anxiety': 4101,\n",
       " 'here': 23545,\n",
       " 'preparing': 40883,\n",
       " 'for': 18653,\n",
       " 'our': 38744,\n",
       " 'deployment': 14058,\n",
       " 'with': 58473,\n",
       " 'grades': 21273,\n",
       " 'in': 25396,\n",
       " 'the': 50602,\n",
       " 'fall': 17464,\n",
       " 'mariaburnham4 wow': 32130,\n",
       " 'wow well': 59290,\n",
       " 'well done': 57430,\n",
       " 'done having': 15323,\n",
       " 'having little': 22763,\n",
       " 'little anxiety': 30528,\n",
       " 'anxiety over': 4296,\n",
       " 'over here': 38976,\n",
       " 'here preparing': 23589,\n",
       " 'preparing for': 40884,\n",
       " 'for our': 18868,\n",
       " 'our deployment': 38755,\n",
       " 'deployment with': 14059,\n",
       " 'with grades': 58567,\n",
       " 'grades in': 21277,\n",
       " 'in the': 25709,\n",
       " 'the fall': 50871,\n",
       " 'lunch': 31527,\n",
       " 'quot': 41681,\n",
       " 'fam': 17483,\n",
       " 'lunch with': 31542,\n",
       " 'with the': 58692,\n",
       " 'the quot': 51243,\n",
       " 'quot fam': 41714,\n",
       " 'fam quot': 17487,\n",
       " 'catching': 9600,\n",
       " 'up': 55394,\n",
       " 'britt': 8322,\n",
       " 'bree': 8223,\n",
       " 'amy': 3034,\n",
       " 'alli': 2345,\n",
       " 'and': 3185,\n",
       " 'steph': 48271,\n",
       " 'ballet': 6025,\n",
       " 'class': 10427,\n",
       " 'now': 36782,\n",
       " 'catching up': 9601,\n",
       " 'up with': 55519,\n",
       " 'with britt': 58512,\n",
       " 'britt bree': 8323,\n",
       " 'bree amy': 8224,\n",
       " 'amy alli': 3035,\n",
       " 'alli and': 2346,\n",
       " 'and steph': 3724,\n",
       " 'steph ballet': 48273,\n",
       " 'ballet class': 6026,\n",
       " 'class now': 10439,\n",
       " 'rt': 43568,\n",
       " 'girlposts': 20411,\n",
       " 'when': 57777,\n",
       " 'you': 60124,\n",
       " 'want': 56611,\n",
       " 'to': 52981,\n",
       " 'be': 6268,\n",
       " 'friends': 19332,\n",
       " 'ppl': 40752,\n",
       " 'but': 8627,\n",
       " 'think': 52129,\n",
       " 'ur': 55590,\n",
       " 'mean': 32770,\n",
       " 'unapproachable': 55212,\n",
       " 'cus': 13106,\n",
       " 'have': 22502,\n",
       " 'resting': 42979,\n",
       " 'bitch': 7488,\n",
       " 'face': 17362,\n",
       " 'soci': 47119,\n",
       " 'rt girlposts': 43789,\n",
       " 'girlposts when': 20415,\n",
       " 'when you': 57872,\n",
       " 'you want': 60497,\n",
       " 'want to': 56632,\n",
       " 'to be': 53033,\n",
       " 'be friends': 6347,\n",
       " 'friends ppl': 19363,\n",
       " 'ppl but': 40755,\n",
       " 'but ppl': 8746,\n",
       " 'ppl think': 40765,\n",
       " 'think ur': 52183,\n",
       " 'ur mean': 55620,\n",
       " 'mean and': 32772,\n",
       " 'and unapproachable': 3789,\n",
       " 'unapproachable cus': 55213,\n",
       " 'cus have': 13108,\n",
       " 'have resting': 22657,\n",
       " 'resting bitch': 42980,\n",
       " 'bitch face': 7491,\n",
       " 'face and': 17363,\n",
       " 'and soci': 3707,\n",
       " 'coconutoilbae': 11668,\n",
       " 'rt coconutoilbae': 43696,\n",
       " 'coconutoilbae when': 11669,\n",
       " 'kaitlinkaboom': 28520,\n",
       " 'hmm': 23981,\n",
       " 'that': 50241,\n",
       " 'would': 59221,\n",
       " 'work': 58971,\n",
       " 'kaitlinkaboom hmm': 28521,\n",
       " 'hmm that': 23990,\n",
       " 'that would': 50563,\n",
       " 'would work': 59251,\n",
       " 'work think': 59024,\n",
       " 'boodlebrain': 7870,\n",
       " 're': 42086,\n",
       " 'wonderful': 58863,\n",
       " 'grandma': 21305,\n",
       " 'punk': 41420,\n",
       " 'akjdhf': 2076,\n",
       " 'feel': 17773,\n",
       " 'ya': 59626,\n",
       " 'on': 37887,\n",
       " 'creator': 12843,\n",
       " 'hoo': 24205,\n",
       " 'boy': 8048,\n",
       " 'fandoms': 17531,\n",
       " 'man': 31962,\n",
       " 'boodlebrain you': 7871,\n",
       " 'you re': 60399,\n",
       " 're wonderful': 42185,\n",
       " 'wonderful grandma': 58869,\n",
       " 'grandma punk': 21307,\n",
       " 'punk and': 41421,\n",
       " 'and akjdhf': 3196,\n",
       " 'akjdhf feel': 2077,\n",
       " 'feel ya': 17811,\n",
       " 'ya on': 59648,\n",
       " 'on that': 38064,\n",
       " 'that creator': 50305,\n",
       " 'creator anxiety': 12844,\n",
       " 'anxiety hoo': 4222,\n",
       " 'hoo boy': 24206,\n",
       " 'boy fandoms': 8054,\n",
       " 'fandoms man': 17532,\n",
       " 'cellix1': 9738,\n",
       " 'opened': 38376,\n",
       " 'anniversary': 3975,\n",
       " 'lootboxes': 31090,\n",
       " 'all': 2197,\n",
       " 'got': 21101,\n",
       " 'was': 56693,\n",
       " 'depression': 14081,\n",
       " 'https': 24665,\n",
       " 'co': 10621,\n",
       " 'm7rnxcphdm': 31662,\n",
       " 'rt cellix1': 43679,\n",
       " 'cellix1 opened': 9739,\n",
       " 'opened anniversary': 38377,\n",
       " 'anniversary lootboxes': 3977,\n",
       " 'lootboxes and': 31091,\n",
       " 'and all': 3199,\n",
       " 'all got': 2241,\n",
       " 'got was': 21181,\n",
       " 'was depression': 56727,\n",
       " 'depression https': 14182,\n",
       " 'https co': 24666,\n",
       " 'co m7rnxcphdm': 11191,\n",
       " 'humansofny': 24731,\n",
       " 'don': 15219,\n",
       " 'going': 20734,\n",
       " 'miss': 33640,\n",
       " 'eighth': 16176,\n",
       " 'grade': 21261,\n",
       " 'been': 6744,\n",
       " 'tough': 54155,\n",
       " 'year': 59851,\n",
       " 'lot': 31154,\n",
       " 'of': 37110,\n",
       " 'my': 34737,\n",
       " 'are': 4668,\n",
       " 'struggling': 48667,\n",
       " 'depr': 14060,\n",
       " 'rt humansofny': 43830,\n",
       " 'humansofny don': 24732,\n",
       " 'don think': 15284,\n",
       " 'think going': 52140,\n",
       " 'going to': 20773,\n",
       " 'to miss': 53330,\n",
       " 'miss eighth': 33644,\n",
       " 'eighth grade': 16177,\n",
       " 'grade it': 21267,\n",
       " 'it been': 26822,\n",
       " 'been tough': 6822,\n",
       " 'tough year': 54160,\n",
       " 'year lot': 59868,\n",
       " 'lot of': 31162,\n",
       " 'of my': 37321,\n",
       " 'my friends': 34906,\n",
       " 'friends are': 19336,\n",
       " 'are struggling': 4794,\n",
       " 'struggling with': 48669,\n",
       " 'with depr': 58536,\n",
       " 'lisarenee123': 30470,\n",
       " 'so': 46883,\n",
       " 'they': 51980,\n",
       " 'not': 36554,\n",
       " 'aging': 1886,\n",
       " 'look': 30992,\n",
       " 'about': 1152,\n",
       " '30': 471,\n",
       " 'or': 38450,\n",
       " 'younger': 60539,\n",
       " 'yay': 59719,\n",
       " 'lisarenee123 so': 30471,\n",
       " 'so they': 47068,\n",
       " 'they re': 52025,\n",
       " 're not': 42150,\n",
       " 'not aging': 36556,\n",
       " 'aging well': 1889,\n",
       " 'well you': 57496,\n",
       " 'you look': 60330,\n",
       " 'look about': 30993,\n",
       " 'about 30': 1154,\n",
       " '30 or': 480,\n",
       " 'or younger': 38567,\n",
       " 'younger so': 60541,\n",
       " 'so yay': 47091,\n",
       " 'yay for': 59722,\n",
       " 'for you': 18994,\n",
       " 'chiller': 10201,\n",
       " 'crisis': 12887,\n",
       " 'mental': 33052,\n",
       " 'health': 22945,\n",
       " 'is': 26251,\n",
       " 'yet': 60026,\n",
       " 'funding': 19714,\n",
       " 'there': 51826,\n",
       " 'take': 49474,\n",
       " 'parents': 39275,\n",
       " 'court': 12675,\n",
       " 'whatever': 57742,\n",
       " 'cost': 12538,\n",
       " 'rt chiller': 43690,\n",
       " 'chiller schools': 10202,\n",
       " 'schools are': 44996,\n",
       " 'are in': 4735,\n",
       " 'in crisis': 25472,\n",
       " 'crisis mental': 12892,\n",
       " 'mental health': 33067,\n",
       " 'health is': 23035,\n",
       " 'is in': 26433,\n",
       " 'crisis yet': 12895,\n",
       " 'yet the': 60043,\n",
       " 'the funding': 50922,\n",
       " 'funding is': 19716,\n",
       " 'is there': 26602,\n",
       " 'there to': 51888,\n",
       " 'to take': 53531,\n",
       " 'take parents': 49494,\n",
       " 'parents to': 39289,\n",
       " 'to court': 53104,\n",
       " 'court whatever': 12678,\n",
       " 'whatever the': 57746,\n",
       " 'the cost': 50780,\n",
       " 'sistren': 46474,\n",
       " 'shaking': 45674,\n",
       " 'yvmhfjbwu1': 60918,\n",
       " 'depression sistren': 14262,\n",
       " 'sistren is': 46475,\n",
       " 'is shaking': 26555,\n",
       " 'shaking https': 45676,\n",
       " 'co yvmhfjbwu1': 11623,\n",
       " 'third': 52228,\n",
       " 'holidays': 24058,\n",
       " 'has': 22351,\n",
       " 'officially': 37582,\n",
       " 'started': 48130,\n",
       " 'play': 40232,\n",
       " 'theme': 51697,\n",
       " 'hospital': 24338,\n",
       " 'best': 7098,\n",
       " 'game': 19875,\n",
       " 'ever': 16901,\n",
       " 'god third': 20699,\n",
       " 'third in': 52230,\n",
       " 'in class': 25461,\n",
       " 'class holidays': 10433,\n",
       " 'holidays has': 24059,\n",
       " 'has officially': 22409,\n",
       " 'officially started': 37587,\n",
       " 'started going': 48132,\n",
       " 'to play': 53397,\n",
       " 'play theme': 40252,\n",
       " 'theme hospital': 51701,\n",
       " 'hospital now': 24340,\n",
       " 'now best': 36796,\n",
       " 'best game': 7113,\n",
       " 'game ever': 19880,\n",
       " 'psuworldcampus': 41307,\n",
       " 'reduce': 42559,\n",
       " 'everyday': 16985,\n",
       " 'stress': 48607,\n",
       " 'through': 52581,\n",
       " 'these': 51933,\n",
       " 'guided': 21678,\n",
       " 'meditation': 32879,\n",
       " 'videos': 56235,\n",
       " 'courtesy': 12682,\n",
       " 'penn_state': 39592,\n",
       " 'kami': 28538,\n",
       " 'dvorakova': 15848,\n",
       " 'rt psuworldcampus': 44065,\n",
       " 'psuworldcampus reduce': 41308,\n",
       " 'reduce everyday': 42564,\n",
       " 'everyday stress': 16990,\n",
       " 'stress and': 48609,\n",
       " 'and anxiety': 3210,\n",
       " 'anxiety through': 4356,\n",
       " 'through these': 52605,\n",
       " 'these guided': 51941,\n",
       " 'guided meditation': 21679,\n",
       " 'meditation videos': 32885,\n",
       " 'videos courtesy': 56238,\n",
       " 'courtesy of': 12683,\n",
       " 'of penn_state': 37349,\n",
       " 'penn_state kami': 39593,\n",
       " 'kami dvorakova': 28539,\n",
       " 'taylorswift13': 49735,\n",
       " 'http': 24618,\n",
       " 'twitpic': 54910,\n",
       " 'com': 11787,\n",
       " '6g790': 782,\n",
       " 'lake': 29387,\n",
       " 'hair': 22043,\n",
       " 'fun': 19653,\n",
       " 'still': 48342,\n",
       " 'very': 56068,\n",
       " 'pretty': 40919,\n",
       " 'always': 2551,\n",
       " 'looking': 31036,\n",
       " 'amazing': 2744,\n",
       " 'taylorswift13 http': 49736,\n",
       " 'http twitpic': 24656,\n",
       " 'twitpic com': 54911,\n",
       " 'com 6g790': 11827,\n",
       " '6g790 lake': 783,\n",
       " 'lake hair': 29388,\n",
       " 'hair is': 22050,\n",
       " 'is not': 26498,\n",
       " 'not fun': 36605,\n",
       " 'fun but': 19662,\n",
       " 'but you': 8811,\n",
       " 'you are': 60153,\n",
       " 'are still': 4793,\n",
       " 'still very': 48408,\n",
       " 'very pretty': 56099,\n",
       " 'pretty you': 40945,\n",
       " 'are always': 4676,\n",
       " 'always looking': 2584,\n",
       " 'looking amazing': 31037,\n",
       " 'spock': 47892,\n",
       " 'eeeis': 16107,\n",
       " 'awesome': 5614,\n",
       " 'spock eeeis': 47893,\n",
       " 'eeeis awesome': 16108,\n",
       " 'bed': 6692,\n",
       " 'qwanoes': 41852,\n",
       " 'tomorrow': 53825,\n",
       " 'met': 33192,\n",
       " 'cit': 10382,\n",
       " 'councellors': 12610,\n",
       " 'goodnight': 21024,\n",
       " 'to bed': 53038,\n",
       " 'bed qwanoes': 6713,\n",
       " 'qwanoes tomorrow': 41853,\n",
       " 'tomorrow met': 53848,\n",
       " 'met my': 33196,\n",
       " 'my cit': 34817,\n",
       " 'cit councellors': 10383,\n",
       " 'councellors goodnight': 12611,\n",
       " 'goodnight everyone': 21025,\n",
       " 'realtrump2016': 42431,\n",
       " 'worst': 59185,\n",
       " 'thing': 52057,\n",
       " 'happen': 22184,\n",
       " 'america': 2795,\n",
       " 'since': 46355,\n",
       " 'great': 21351,\n",
       " 'complete': 12100,\n",
       " 'moron': 34238,\n",
       " 'realtrump2016 you': 42432,\n",
       " 'are the': 4799,\n",
       " 'the worst': 51497,\n",
       " 'worst thing': 59197,\n",
       " 'thing to': 52091,\n",
       " 'to happen': 53217,\n",
       " 'happen to': 22187,\n",
       " 'to america': 53005,\n",
       " 'america since': 2799,\n",
       " 'since the': 46374,\n",
       " 'the great': 50955,\n",
       " 'great depression': 21365,\n",
       " 'depression you': 14318,\n",
       " 'are complete': 4694,\n",
       " 'complete moron': 12104,\n",
       " 'nicamora': 36055,\n",
       " 'finna': 18207,\n",
       " 'asleep': 5146,\n",
       " 'seconds': 45156,\n",
       " 'why': 58135,\n",
       " 'creatives': 12838,\n",
       " 'seem': 45285,\n",
       " 'experience': 17217,\n",
       " 'most': 34249,\n",
       " 'rt nicamora': 44025,\n",
       " 'nicamora finna': 36056,\n",
       " 'finna be': 18208,\n",
       " 'be asleep': 6284,\n",
       " 'asleep in': 5150,\n",
       " 'in seconds': 25665,\n",
       " 'seconds but': 45157,\n",
       " 'but why': 8800,\n",
       " 'why is': 58157,\n",
       " 'is it': 26439,\n",
       " 'it that': 27076,\n",
       " 'that creatives': 50304,\n",
       " 'creatives seem': 12839,\n",
       " 'seem to': 45291,\n",
       " 'to experience': 53168,\n",
       " 'experience depression': 17219,\n",
       " 'depression the': 14285,\n",
       " 'the most': 51118,\n",
       " 'botanical': 7972,\n",
       " 'he': 22805,\n",
       " 'spent': 47827,\n",
       " 'two': 55025,\n",
       " 'years': 59880,\n",
       " 'planting': 40218,\n",
       " 'thousands': 52545,\n",
       " 'scented': 44931,\n",
       " 'flowers': 18431,\n",
       " 'his': 23897,\n",
       " 'blind': 7641,\n",
       " 'wife': 58198,\n",
       " 'smell': 46766,\n",
       " 'help': 23362,\n",
       " 'her': 23467,\n",
       " 'rt botanical': 43659,\n",
       " 'botanical he': 7973,\n",
       " 'he spent': 22870,\n",
       " 'spent two': 47835,\n",
       " 'two years': 55048,\n",
       " 'years planting': 59899,\n",
       " 'planting thousands': 40219,\n",
       " 'thousands of': 52546,\n",
       " 'of scented': 37388,\n",
       " 'scented flowers': 44932,\n",
       " 'flowers for': 18433,\n",
       " 'for his': 18773,\n",
       " 'his blind': 23902,\n",
       " 'blind wife': 7642,\n",
       " 'wife to': 58203,\n",
       " 'to smell': 53496,\n",
       " 'smell to': 46769,\n",
       " 'to help': 53231,\n",
       " 'help with': 23414,\n",
       " 'with her': 58574,\n",
       " 'her depression': 23486,\n",
       " 'feeling': 17815,\n",
       " 'like': 30187,\n",
       " 'an': 3046,\n",
       " 'elephant': 16221,\n",
       " 'sitting': 46504,\n",
       " 'your': 60552,\n",
       " 'chest': 10107,\n",
       " 'being': 6916,\n",
       " 'able': 1144,\n",
       " 'breathe': 8215,\n",
       " 'depression is': 14189,\n",
       " 'is feeling': 26368,\n",
       " 'feeling like': 17823,\n",
       " 'like an': 30196,\n",
       " 'an elephant': 3093,\n",
       " 'elephant is': 16222,\n",
       " 'is sitting': 26565,\n",
       " 'sitting on': 46508,\n",
       " 'on your': 38110,\n",
       " 'your chest': 60588,\n",
       " 'chest and': 10108,\n",
       " 'and not': 3589,\n",
       " 'not being': 36572,\n",
       " 'being able': 6921,\n",
       " 'able to': 1145,\n",
       " 'to breathe': 53056,\n",
       " 'lifeaseva': 30151,\n",
       " 'respect': 42937,\n",
       " 'acknowledge': 1399,\n",
       " 'people': 39603,\n",
       " 'problems': 41091,\n",
       " 'real': 42259,\n",
       " 'something': 47377,\n",
       " 'anyone': 4471,\n",
       " 'chooses': 10271,\n",
       " 'live': 30567,\n",
       " 'rt lifeaseva': 43935,\n",
       " 'lifeaseva respect': 30156,\n",
       " 'respect and': 42938,\n",
       " 'and acknowledge': 3188,\n",
       " 'acknowledge people': 1400,\n",
       " 'people with': 39683,\n",
       " 'with mental': 58620,\n",
       " 'health problems': 23072,\n",
       " 'problems it': 41101,\n",
       " 'it is': 26937,\n",
       " 'is real': 26533,\n",
       " 'real thing': 42289,\n",
       " 'thing and': 52059,\n",
       " 'not something': 36678,\n",
       " 'something anyone': 47380,\n",
       " 'anyone chooses': 4473,\n",
       " 'chooses to': 10272,\n",
       " 'to live': 53296,\n",
       " 'live with': 30590,\n",
       " 'oh': 37616,\n",
       " 'one': 38135,\n",
       " 'more': 34058,\n",
       " 'same': 44554,\n",
       " 'rocks': 43371,\n",
       " 'no': 36328,\n",
       " 'frickenn': 19276,\n",
       " 'other': 38664,\n",
       " 'favorite': 17663,\n",
       " 'song': 47449,\n",
       " 'oh and': 37618,\n",
       " 'and one': 3598,\n",
       " 'one more': 38181,\n",
       " 'more thing': 34136,\n",
       " 'thing quot': 52082,\n",
       " 'quot one': 41769,\n",
       " 'one and': 38139,\n",
       " 'and the': 3762,\n",
       " 'the same': 51282,\n",
       " 'same quot': 44582,\n",
       " 'quot rocks': 41786,\n",
       " 'rocks like': 43372,\n",
       " 'like no': 30276,\n",
       " 'no frickenn': 36354,\n",
       " 'frickenn other': 19277,\n",
       " 'other favorite': 38672,\n",
       " 'favorite song': 17671,\n",
       " 'dear': 13793,\n",
       " 'if': 25006,\n",
       " 'love': 31199,\n",
       " 'me': 32518,\n",
       " 'how': 24465,\n",
       " 'stop': 48449,\n",
       " 'causing': 9662,\n",
       " 'much': 34520,\n",
       " 'pain': 39158,\n",
       " 'yours': 60801,\n",
       " 'truly': 54521,\n",
       " 'jxn8umfts0': 28488,\n",
       " 'dear anxiety': 13794,\n",
       " 'anxiety if': 4227,\n",
       " 'if you': 25071,\n",
       " 'you love': 60336,\n",
       " 'love me': 31259,\n",
       " 'me how': 32619,\n",
       " 'how about': 24466,\n",
       " 'about you': 1268,\n",
       " 'you stop': 60440,\n",
       " 'stop causing': 48454,\n",
       " 'causing so': 9665,\n",
       " 'so much': 47006,\n",
       " 'much pain': 34568,\n",
       " 'pain yours': 39168,\n",
       " 'yours truly': 60811,\n",
       " 'truly https': 54524,\n",
       " 'co jxn8umfts0': 11125,\n",
       " 'wiiskey': 58212,\n",
       " 'im': 25188,\n",
       " 'hanging': 22163,\n",
       " 'out': 38819,\n",
       " 'lovely': 31346,\n",
       " 'daughter': 13502,\n",
       " 'right': 43163,\n",
       " 'actually': 1470,\n",
       " 'wiiskey im': 58213,\n",
       " 'im hanging': 25219,\n",
       " 'hanging out': 22167,\n",
       " 'out with': 38911,\n",
       " 'with your': 58725,\n",
       " 'your lovely': 60676,\n",
       " 'lovely daughter': 31350,\n",
       " 'daughter right': 13504,\n",
       " 'right now': 43180,\n",
       " 'now actually': 36783,\n",
       " 'jemdevenish': 27606,\n",
       " 'emailed': 16313,\n",
       " 'yooh': 60109,\n",
       " 'jemdevenish emailed': 27607,\n",
       " 'emailed yooh': 16315,\n",
       " 'mmalkoff': 33783,\n",
       " 'visiting': 56316,\n",
       " 'website': 57254,\n",
       " 'first': 18234,\n",
       " 'time': 52765,\n",
       " 'see': 45186,\n",
       " 'what': 57615,\n",
       " 'mmalkoff now': 33784,\n",
       " 'now visiting': 36901,\n",
       " 'visiting your': 56317,\n",
       " 'your website': 60777,\n",
       " 'website for': 57257,\n",
       " 'for the': 18950,\n",
       " 'the first': 50893,\n",
       " 'first time': 18263,\n",
       " 'time to': 52827,\n",
       " 'to see': 53472,\n",
       " 'see what': 45237,\n",
       " 'what on': 57694,\n",
       " 'on there': 38069,\n",
       " 'last': 29476,\n",
       " 'minute': 33598,\n",
       " 'items': 27144,\n",
       " 'before': 6846,\n",
       " 'leave': 29783,\n",
       " 'procrastination': 41127,\n",
       " 'motion': 34310,\n",
       " 'lambs': 29409,\n",
       " 'last minute': 29495,\n",
       " 'minute items': 33602,\n",
       " 'items before': 27145,\n",
       " 'before leave': 6861,\n",
       " 'leave procrastination': 29792,\n",
       " 'procrastination in': 41129,\n",
       " 'in motion': 25596,\n",
       " 'motion lambs': 34312,\n",
       " 'myrandabrown_': 35239,\n",
       " 'find': 18121,\n",
       " 'someone': 47331,\n",
       " 'doesnt': 15124,\n",
       " 'only': 38249,\n",
       " 'good': 20861,\n",
       " 'wants': 56646,\n",
       " 'mind': 33497,\n",
       " 'amp': 2845,\n",
       " 'physical': 39945,\n",
       " 'rt myrandabrown_': 44006,\n",
       " 'myrandabrown_ find': 35240,\n",
       " 'find someone': 18132,\n",
       " 'someone that': 47361,\n",
       " 'that doesnt': 50316,\n",
       " 'doesnt only': 15125,\n",
       " 'only want': 38304,\n",
       " 'to look': 53299,\n",
       " 'look good': 31003,\n",
       " 'good but': 20880,\n",
       " 'but wants': 8793,\n",
       " 'wants ur': 56654,\n",
       " 'ur mind': 55622,\n",
       " 'mind right': 33512,\n",
       " 'right amp': 43164,\n",
       " 'amp for': 2895,\n",
       " 'for to': 18956,\n",
       " 'be good': 6357,\n",
       " 'good god': 20907,\n",
       " 'god physical': 20694,\n",
       " 'physical mental': 39954,\n",
       " 'wish': 58424,\n",
       " 'adri': 1619,\n",
       " 'woke': 58792,\n",
       " 'this': 52235,\n",
       " 'happy': 22208,\n",
       " 'cute': 13132,\n",
       " 'smile': 46778,\n",
       " 'walked': 56545,\n",
       " 'room': 43471,\n",
       " 'wish adri': 58426,\n",
       " 'adri woke': 1620,\n",
       " 'woke up': 58793,\n",
       " 'up this': 55499,\n",
       " 'this happy': 52293,\n",
       " 'happy everyday': 22235,\n",
       " 'everyday so': 16989,\n",
       " 'so cute': 46915,\n",
       " 'cute to': 13158,\n",
       " 'see her': 45198,\n",
       " 'her little': 23505,\n",
       " 'little smile': 30558,\n",
       " 'smile when': 46789,\n",
       " 'when walked': 57861,\n",
       " 'walked in': 56547,\n",
       " 'in her': 25528,\n",
       " 'her room': 23518,\n",
       " 'missemilyjane42': 33673,\n",
       " 'do': 14957,\n",
       " 'missemilyjane42 do': 33674,\n",
       " 'do think': 15025,\n",
       " 'think so': 52169,\n",
       " 'zidizei': 60995,\n",
       " '6m41p': 791,\n",
       " 'kurt': 29269,\n",
       " 'cobain': 11657,\n",
       " 'style': 48741,\n",
       " 'becomes': 6684,\n",
       " 'roger': 43399,\n",
       " 'federer': 17758,\n",
       " 'zidizei http': 60996,\n",
       " 'com 6m41p': 11828,\n",
       " '6m41p kurt': 792,\n",
       " 'kurt cobain': 29270,\n",
       " 'cobain style': 11658,\n",
       " 'style becomes': 48742,\n",
       " 'becomes roger': 6685,\n",
       " 'roger federer': 43400,\n",
       " 'federer style': 17759,\n",
       " 'helengilburt': 23310,\n",
       " 'never': 35790,\n",
       " 'prominent': 41215,\n",
       " 'politics': 40507,\n",
       " 'parties': 39347,\n",
       " 'offering': 37553,\n",
       " 'will': 58223,\n",
       " 'fund': 19709,\n",
       " 'rt helengilburt': 43814,\n",
       " 'helengilburt never': 23311,\n",
       " 'never has': 35814,\n",
       " 'has mental': 22398,\n",
       " 'health been': 22961,\n",
       " 'been so': 6809,\n",
       " 'so prominent': 47027,\n",
       " 'prominent in': 41216,\n",
       " 'in politics': 25632,\n",
       " 'politics but': 40509,\n",
       " 'but what': 8797,\n",
       " 'what are': 57622,\n",
       " 'the parties': 51186,\n",
       " 'parties offering': 39349,\n",
       " 'offering and': 37555,\n",
       " 'and how': 3455,\n",
       " 'how will': 24561,\n",
       " 'will they': 58303,\n",
       " 'they fund': 52002,\n",
       " 'fund it': 19710,\n",
       " 'it https': 26930,\n",
       " 'drdenisemd': 15558,\n",
       " 'instagram': 25996,\n",
       " 'harmful': 22324,\n",
       " 'app': 4573,\n",
       " 'ht4wkdmtbc': 24612,\n",
       " 'via': 56141,\n",
       " 'thescope': 51930,\n",
       " 'rt drdenisemd': 43730,\n",
       " 'drdenisemd instagram': 15561,\n",
       " 'instagram is': 26000,\n",
       " 'is the': 26600,\n",
       " 'most harmful': 34262,\n",
       " 'harmful app': 22325,\n",
       " 'app for': 4574,\n",
       " 'for mental': 18825,\n",
       " 'health https': 23025,\n",
       " 'co ht4wkdmtbc': 11045,\n",
       " 'ht4wkdmtbc via': 24613,\n",
       " 'via thescope': 56169,\n",
       " 'yea': 59762,\n",
       " 'know': 29067,\n",
       " 'due': 15776,\n",
       " 'social': 47121,\n",
       " 'cant': 9345,\n",
       " 'even': 16835,\n",
       " 'make': 31839,\n",
       " 'conversation': 12381,\n",
       " 'link': 30429,\n",
       " 'dont': 15368,\n",
       " 'thin': 52054,\n",
       " '641y6y8d2g': 737,\n",
       " 'yea know': 59773,\n",
       " 'know what': 29123,\n",
       " 'what mean': 57685,\n",
       " 'mean but': 32775,\n",
       " 'but due': 8663,\n",
       " 'due to': 15778,\n",
       " 'to social': 53498,\n",
       " 'social anxiety': 47122,\n",
       " 'anxiety cant': 4149,\n",
       " 'cant even': 9351,\n",
       " 'even make': 16850,\n",
       " 'make the': 31879,\n",
       " 'the right': 51266,\n",
       " 'right conversation': 43169,\n",
       " 'conversation link': 12387,\n",
       " 'link or': 30437,\n",
       " 'or whatever': 38559,\n",
       " 'whatever dont': 57744,\n",
       " 'dont thin': 15389,\n",
       " 'thin https': 52055,\n",
       " 'co 641y6y8d2g': 10720,\n",
       " 'wes10': 57588,\n",
       " 'animals': 3949,\n",
       " 'wes10 do': 57589,\n",
       " 'do you': 15039,\n",
       " 'you think': 60461,\n",
       " 'think that': 52175,\n",
       " 'that your': 50568,\n",
       " 'your animals': 60558,\n",
       " 'animals have': 3951,\n",
       " 'have depression': 22556,\n",
       " 'djmissfrenchie': 14913,\n",
       " 'torontoviewer': 54096,\n",
       " 'she': 45770,\n",
       " 'used': 55759,\n",
       " 'litter': 30525,\n",
       " 'tray': 54349,\n",
       " 'seems': 45298,\n",
       " 'getting': 20238,\n",
       " 'bearings': 6536,\n",
       " 'cat': 9575,\n",
       " 'toilet': 53768,\n",
       " 'updates': 55543,\n",
       " 'djmissfrenchie torontoviewer': 14914,\n",
       " 'torontoviewer she': 54097,\n",
       " 'she has': 45799,\n",
       " 'has now': 22406,\n",
       " 'now used': 36898,\n",
       " 'used the': 55762,\n",
       " 'the litter': 51064,\n",
       " 'litter tray': 30526,\n",
       " 'tray and': 54350,\n",
       " 'and seems': 3687,\n",
       " 'seems to': 45303,\n",
       " 'be getting': 6355,\n",
       " 'getting her': 20253,\n",
       " 'her bearings': 23475,\n",
       " 'bearings more': 6537,\n",
       " 'more cat': 34069,\n",
       " 'cat toilet': 9580,\n",
       " 'toilet updates': 53772,\n",
       " 'updates tomorrow': 55552,\n",
       " 'wanna': 56592,\n",
       " 'resolution': 42927,\n",
       " 'visibly': 56303,\n",
       " 'unashamedly': 55214,\n",
       " 'queer': 41605,\n",
       " 'month': 33997,\n",
       " 'around': 4883,\n",
       " 'bein': 6914,\n",
       " 'seen': 45305,\n",
       " 'as': 4971,\n",
       " 'wanna make': 56604,\n",
       " 'make resolution': 31872,\n",
       " 'resolution to': 42928,\n",
       " 'be visibly': 6489,\n",
       " 'visibly unashamedly': 56304,\n",
       " 'unashamedly queer': 55215,\n",
       " 'queer this': 41606,\n",
       " 'this month': 52322,\n",
       " 'month but': 34002,\n",
       " 'but have': 8691,\n",
       " 'have lot': 22611,\n",
       " 'of anxiety': 37132,\n",
       " 'anxiety around': 4121,\n",
       " 'around bein': 4885,\n",
       " 'bein seen': 6915,\n",
       " 'seen as': 45306,\n",
       " 'as much': 5032,\n",
       " 'much as': 34527,\n",
       " 'as want': 5065,\n",
       " 'millionairemoms': 33478,\n",
       " 'alright': 2466,\n",
       " 'long': 30943,\n",
       " 'buys': 8844,\n",
       " 'chunky': 10336,\n",
       " 'monkey': 33979,\n",
       " 'future': 19772,\n",
       " 'll': 30642,\n",
       " 'than': 50074,\n",
       " 'contented': 12345,\n",
       " 'millionairemoms that': 33479,\n",
       " 'that alright': 50249,\n",
       " 'alright as': 2467,\n",
       " 'as long': 5023,\n",
       " 'long as': 30944,\n",
       " 'as someone': 5051,\n",
       " 'someone buys': 47335,\n",
       " 'buys me': 8845,\n",
       " 'me chunky': 32564,\n",
       " 'chunky monkey': 10337,\n",
       " 'monkey in': 33982,\n",
       " 'in future': 25510,\n",
       " 'future ll': 19777,\n",
       " 'll be': 30649,\n",
       " 'be more': 6401,\n",
       " 'more than': 34134,\n",
       " 'than contented': 50085,\n",
       " 'a_dorkable': 1088,\n",
       " 'guadiance': 21615,\n",
       " 'lets': 29997,\n",
       " 'go': 20583,\n",
       " 'mcdoodle': 32479,\n",
       " 'a_dorkable guadiance': 1089,\n",
       " 'guadiance lets': 21616,\n",
       " 'lets go': 30000,\n",
       " 'go to': 20660,\n",
       " 'to mcdoodle': 53313,\n",
       " 'get': 20093,\n",
       " 'dad': 13242,\n",
       " 'another': 4005,\n",
       " 'tie': 52695,\n",
       " 'father': 17638,\n",
       " 'day': 13558,\n",
       " 'let': 29958,\n",
       " 'us': 55677,\n",
       " 'bit': 7464,\n",
       " 'ly': 31584,\n",
       " 'meqgk': 33146,\n",
       " 'don get': 15241,\n",
       " 'get your': 20206,\n",
       " 'your dad': 60600,\n",
       " 'dad another': 13244,\n",
       " 'another tie': 4036,\n",
       " 'tie this': 52697,\n",
       " 'this father': 52277,\n",
       " 'father day': 17639,\n",
       " 'day let': 13621,\n",
       " 'let us': 29993,\n",
       " 'us help': 55696,\n",
       " 'help http': 23377,\n",
       " 'http bit': 24621,\n",
       " 'bit ly': 7474,\n",
       " 'ly meqgk': 31610,\n",
       " 'kyle1092': 29289,\n",
       " 'less': 29932,\n",
       " 'trying': 54589,\n",
       " 'naps': 35435,\n",
       " 'yes': 59948,\n",
       " 'friend': 19301,\n",
       " 'kyle1092 im': 29290,\n",
       " 'im more': 25234,\n",
       " 'more or': 34119,\n",
       " 'or less': 38511,\n",
       " 'less trying': 29947,\n",
       " 'trying to': 54595,\n",
       " 'to stop': 53515,\n",
       " 'stop depression': 48457,\n",
       " 'depression naps': 14222,\n",
       " 'naps but': 35437,\n",
       " 'but yes': 8810,\n",
       " 'yes thank': 59986,\n",
       " 'thank you': 50139,\n",
       " 'you friend': 60262,\n",
       " 'steveweber': 48309,\n",
       " 'agree': 1899,\n",
       " 'whole': 58090,\n",
       " 'heartedly': 23221,\n",
       " 'article': 4946,\n",
       " 'steveweber agree': 48310,\n",
       " 'agree with': 1909,\n",
       " 'with you': 58724,\n",
       " 'you whole': 60509,\n",
       " 'whole heartedly': 58098,\n",
       " 'heartedly great': 23222,\n",
       " 'great article': 21355,\n",
       " 'hoecry': 24021,\n",
       " 'wcw': 57117,\n",
       " 'sleeps': 46670,\n",
       " 'hours': 24406,\n",
       " 'at': 5208,\n",
       " 'night': 36163,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = all_tweets.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, targets_train, targets_test = train_test_split(indexed_data, targets, test_size=0.3, random_state=0)\n",
    "data_train_index = data_train[:,0]\n",
    "data_train = data_train[:,1:]\n",
    "data_test_index = data_test[:,0]\n",
    "data_test = data_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=1, kernel='linear')\n",
    "clf_output = clf.fit(data_train, targets_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9948717948717949"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_output.score(data_test, targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRID-SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.5, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.9974015590645613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "hiperparameters = {'kernel': ('linear', 'rbf','poly'), \n",
    "              'C':[1.5, 10, 100, 1000],\n",
    "              'gamma': [1e-7, 1e-4, 1e-2, 0.1]}\n",
    "grid = GridSearchCV(svm.SVC(), hiperparameters, cv=5)\n",
    "grid.fit(data_train, targets_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# add this kernels polynominal, sigmoid\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(data_train, targets_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = targets_test, clf.predict(data_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_to_predict = ['I have so many reasons to smile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = normalizer(tweet_to_predict)\n",
    "#text = count_vectorizer.fit_transform(normalizer(tweet_to_predict))\n",
    "#vectorized_data = count_vectorizer.fit_transform(all_tweets.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#text = count_vectorizer.transform(normalizer(tweet_to_predict))\n",
    "#unseen = (np.array(range(0,vectorized_data.shape[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_output.predict(unseen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
